# 1081 專案期末報告 12/29
# 環境建置
## 下載CUDA 9.0
![](https://i.imgur.com/VZ42AES.png)
照著教學內容的步驟走，遇到以下的狀況：
![](https://i.imgur.com/lGO9VNF.png)
## CUDA安裝失敗的解決方法
但不同於圖片上顯示的VScode外掛安裝失敗，我們遇到的問題是NVIDIA Update Core的安裝失敗，搜尋解決方法後，決定更新windows系統到1903版，更新後即可順利安裝CUDA。
接下來我們依照教學，將cuDNN下載後解壓縮，並將bin, include, lib這三個資料夾複製到CUDA 9.0的資料夾內，並且開一個屬於tensorflow-gpu的anaconda environment(我們取名叫做：tfenv)。
## 找不到nvcc -V的解決方法
在安裝好CUDA與cuDNN之後，為了檢查CUDA是否有正確安裝，可以對終端機下`nvcc -V`的指令確認CUDA版本。但在剛裝好時得到了`'nvcc' 不是內部或外部命令、可執行的程式或批次檔。`的錯誤，但重開機後就很神奇地被解決了。
## DLL找不到模組的解決方法
在解決上述所有問題之後，我們在conda prompt內嘗試import tensorflow，結果得到了DLL找不到模組的錯誤。因此，我們將所有的tensorflow都先`pip uninstall`(注意不是`conda install`)，重新安裝後就沒有再出錯了。

## Reference
https://medium.com/%E9%9B%9E%E9%9B%9E%E8%88%87%E5%85%94%E5%85%94%E7%9A%84%E5%B7%A5%E7%A8%8B%E4%B8%96%E7%95%8C/%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-ml-note-windows-%E6%90%AD%E5%BB%BAtensorflow-gpu-%E7%92%B0%E5%A2%83-anaconda-tensorflow-gpu-cuda-cudnn-a047c0f275f4

# Website to Image
## 整理Malicious Website的資料集
我們在Kaggle上找到了一個Malicious and Benignc Websites的Dataset，不過這個Dataset裡的連結欄位全都是代號，於是我們參考了作者提供的參考連結，下載了總共7個含有惡意/善意網頁URL的csv檔；而這7個檔案的欄位格式都不太相同，以下為我們修改的程式碼：
### 讀入檔案
```python=
import pandas as pd
import numpy as np
export1 = pd.read_csv('export.csv', encoding='big5')
export3 = pd.read_csv('export (3).csv', encoding='big5')
faizann = pd.read_csv('faizann24-malicious.csv', encoding='big5')
hiddenURLs = pd.read_csv('HiddenFraudulentURLs.csv', error_bad_lines=False)
sslblacklist = pd.read_csv('sslblacklist-botnet.csv', encoding='big5')
updates = pd.read_csv('updates.csv', encoding='big5')
yesterday_urls = pd.read_csv('yesterday_urls.csv', encoding='big5')
```
### export1 查看欄位、並且把多餘的欄位去除
```python=
export1.columns = ['date', 'url', 'ip', 'url2', 'malware type', '-', '--', '---', 'region', '----']
del export1['date']
del export1['-']
del export1['--']
del export1['---']
del export1['region']
del export1['----']
# 因為export1和export3的內容完全一樣，所以我們捨去了export3
```
### hiddenURLs 整理欄位
```python=
hiddenURLsColumns = np.array(hiddenURLs.columns[0].split(';'))
hiddenURLsItems = hiddenURLs.index
type(hiddenURLsItems)    #pandas.core.indexes.base.Index
dataToHidden = [hiddenURLsColumns]
for each in hiddenURLsItems:
    dataToAppend = each.split(';')    #這個csv內的每筆資料是以分號隔開
    dataToHidden.append(np.array(dataToAppend))
hiddenURLs = pd.DataFrame(dataToHidden)

# 清除多餘欄位
for i in range(8, 22):
    del hiddenURLs[i]
    
# 設定新的Columns
hiddenURLs.columns = hiddenURLsColumns

#刪除多餘的資料
hiddenURLs.drop(index=0)
```
### sslblacklist 整理欄位
```python=
sslblacklist.head()
>>>                    Firstseen,DstIP,DstPort
    0	2019-12-13 19:11:57,172.247.227.11,4782
    1	2019-12-13 11:57:49,31.192.109.47,443
    2	2019-12-13 03:50:25,185.244.30.244,2211
    3	2019-12-13 00:58:31,104.27.181.27,443
    4	2019-12-12 22:50:13,79.134.225.82,1112
    #只有一個欄位，且這個csv內的每筆資料是以逗號隔開
    
#切割資料
sslblacklistColumns = sslblacklist.columns[0].split(',')
sslblacklistColumns[0] = sslblacklistColumns[0][2:]

#確認欄位
sslblacklistColumns
>>> ['Firstseen', 'DstIP', 'DstPort']

#將所有資料存到sslblacklistItems這個變數上
sslblacklist.columns = ['---']
sslblacklistItems = sslblacklist['---']

#切割資料
dataToSSL = []
for each in sslblacklistItems:
    data = each.split(',')[1:]
    dataToSSL.append(np.array(data))
    
#只取原本Columns的後兩個index(DstIP & DstPort)
sslblacklist = pd.DataFrame(dataToSSL)
sslblacklist.columns = sslblacklistColumns[1:]
```
### faizann 的資料切割
```python=
faizann = pd.read_csv('faizann24-malicious.csv', encoding='big5')
faizann.head()
>>>                     url,label
    0  diaryofagameaddict.com,bad
    1        espdesign.com.au,bad
    2      iamagameaddict.com,bad
    3           kalantzis.net,bad
    4   slightlyoffcenter.net,bad 
    #這個csv內的每筆資料是以逗號隔開

#將所有資料存到faizannItems上
faizann.columns = ['-']
faizannItems = faizann['-']

#將url與label做切割，並轉成DataFrame
dataToFai = []
for each in faizannItems:
    try:
        idx = each.rfind('bad')
        data = np.array([each[:idx-1], each[idx:]])
        dataToFai.append(data)
    except:
        print(each)
faizann = pd.DataFrame(dataToFai)
```
### 合併所有DataFrames
```python=
# 確認所有DataFrames欄位
for each in allDFs:
    print(each.columns)
>>> Index(['url', 'ip', 'url2', 'malware type'], dtype='object')
    Index(['url', 'label'], dtype='object')
    Index(['url', 'compromissionType', 'isHiddenFraudulent', 'contentLength',
           'serverType', 'poweredBy', 'contentType', 'lastModified'],
          dtype='object')
    Index(['DstIP', 'DstPort'], dtype='object')
    
# 刪除用不到的欄位
del export1['url']
del export1['url2']
hiddenURLs = hiddenURLs.drop(faizann.index[0])
del hiddenURLs['isHiddenFraudulent']
del hiddenURLs['contentLength']
del hiddenURLs['serverType']
del hiddenURLs['poweredBy']
del hiddenURLs['contentType']
del hiddenURLs['lastModified']
sslblacklist['IP'] = sslblacklist['DstIP']+':'+sslblacklist['DstPort']
del sslblacklist['DstIP']
del sslblacklist['DstPort']

#Concat所有DataFrames
allDFs = [export1, faizann, hiddenURLs, sslblacklist]
for each in allDFs:
    try:
        each.columns = ['url', 'label']
    except:
        each.columns = ['url']
    print(each.columns)
    
# 匯出 
allData = pd.concat([export1,faizann,hiddenURLs,sslblacklist],axis=0)
allData.to_csv('allData.csv')
```

## Reference
https://www.kaggle.com/xwolf12/malicious-and-benign-websites
https://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs
https://abuse.ch/
https://machinelearning.inginf.units.it/data-and-tools/hidden-fraudulent-urls-dataset
http://www.malwaredomainlist.com/forums/index.php?topic=3270.0

# 堯堯的部分

# New Dataset
As urls of malicious websites which are mentioned above are blocked by transfering websites, we choose other style and dataset to transfer style. One is normal faces, another is faces of chracthers in Simpson family. We decide transfer human faces into Simpson, so that we can make fun of it. Here are urls and examples we download.

references: 

Normal face: 
https://www.kaggle.com/varump66/face-images-13233
![](https://i.imgur.com/cBWGe5b.jpg)
Simpson:
https://www.kaggle.com/kostastokis/simpsons-faces
![](https://i.imgur.com/SdFyzVu.png)

# 潘
# CycleGAN

# 